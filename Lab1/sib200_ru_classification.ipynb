{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Настройка макета записаной книжки -> Output: Word Wrap -> Вкл.**\n",
        "\n",
        "### Что изменено?\n",
        "- ХЗ, пока вроде ничего, просто код почище сделал"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c9jiCwnlYu0",
        "outputId": "c2a65115-897b-4622-e9c8-066fe33eb346"
      },
      "outputs": [],
      "source": [
        "# РОТ ШАТАЛ ЭТИХ ЗАВИСИМОСТЕЙ ПИТОНА\n",
        "\n",
        "# !sudo apt remove python* -y\n",
        "# !sudo apt install python3 pip -y\n",
        "# !/bin/python3 -m pip install ipykernel -U --user --force-reinstall\n",
        "# !pip freeze | cut -d \"@\" -f1 | xargs pip uninstall -y\n",
        "\n",
        "# !pip uninstall datasets spacy scikit-learn -y\n",
        "# !pip uninstall thinc numpy scipy -y\n",
        "# !pip install \"thinc>8.3\" \"numpy<2.0\" scipy\n",
        "# !pip install scikit-learn spacy datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLvXbwL7mUbB",
        "outputId": "7fd29dcc-1e2e-4d5a-8197-2e176212feb8"
      },
      "outputs": [],
      "source": [
        "!python3 -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "xeVQ_Pk3b0ve"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import spacy\n",
        "from datasets import load_dataset\n",
        "import multiprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Загрузим и разделим текстовый корпус"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "sqxpYOORb9iM"
      },
      "outputs": [],
      "source": [
        "def load_sib200_ru() -> Tuple[Tuple[List[str], List[int]], Tuple[List[str], List[int]], Tuple[List[str], List[int]], List[str]]:\n",
        "    trainset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='train')\n",
        "    X_train = trainset['text']\n",
        "    y_train = trainset['category']\n",
        "\n",
        "    valset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='validation')\n",
        "    X_val = valset['text']\n",
        "    y_val = valset['category']\n",
        "\n",
        "    testset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='test')\n",
        "    X_test = testset['text']\n",
        "    y_test = testset['category']\n",
        "\n",
        "\n",
        "    categories = set(y_train)\n",
        "\n",
        "    unknown_categories = set(y_val) - set(y_train)\n",
        "    if len(unknown_categories) > 0:\n",
        "        raise RuntimeError(\n",
        "            f'Категории {unknown_categories} есть в валидационном наборе, но не в тренировочном'\n",
        "        )\n",
        "\n",
        "    unknown_categories = set(y_test) - set(y_train)\n",
        "    if len(unknown_categories) > 0:\n",
        "        raise RuntimeError(\n",
        "            f'Категории {unknown_categories} есть в тестовом наборе, но не в тренировочном'\n",
        "        )\n",
        "\n",
        "\n",
        "    categories = sorted(list(categories))\n",
        "\n",
        "    y_train = [categories.index(it) for it in y_train]\n",
        "    y_val = [categories.index(it) for it in y_val]\n",
        "    y_test = [categories.index(it) for it in y_test]\n",
        "\n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo8KuNe9kX_H",
        "outputId": "52d3b096-ac46-4cba-9c3f-4cfb58f71e32"
      },
      "outputs": [],
      "source": [
        "train_data, val_data, test_data, classes_list = load_sib200_ru()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVIr5afvzWut",
        "outputId": "4b8db3bf-d373-48da-c82a-c43dbf923449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Категории текстового корпуса: ['entertainment', 'geography', 'health', 'politics', 'science/technology', 'sports', 'travel']\n"
          ]
        }
      ],
      "source": [
        "print('Категории текстового корпуса:', classes_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U-LEbKKsTaJ",
        "outputId": "fe7a80b3-2569-41cd-ab67-7c5c4a05b22d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "701 701\n",
            "99 99\n",
            "204 204\n"
          ]
        }
      ],
      "source": [
        "print(len(train_data[0]), len(train_data[1]))\n",
        "print(len(val_data[0]), len(val_data[1]))\n",
        "print(len(test_data[0]), len(test_data[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Применим SpaCy для предобработки текста"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_text(s: str, nlp_pipeline: spacy.Language) -> str:\n",
        "    doc = nlp_pipeline(s)\n",
        "    lemmas = [('<NUM>' if token.like_num else token.lemma_.lower()) for token in filter(lambda it1: not it1.is_punct, doc)]\n",
        "    if len(lemmas) == 0:\n",
        "        return ''\n",
        "    return ' '.join(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "0mENTDYFl7bz"
      },
      "outputs": [],
      "source": [
        "NLP_PIPELINE = spacy.load('ru_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Турция с трёх сторон окружена морями: на западе — Эгейским, на севере — Чёрным и на юге — Средиземным. \n",
            "турция с <NUM> сторона окружить морями на запад эгейским на север чёрный и на юг средиземный\n",
            "\n",
            "Если увеличить расстояние для бега с четверти до половины мили, скорость становится не так важна, тогда как выносливость превращается в абсолютную необходимость. \n",
            "если увеличить расстояние для бег с <NUM> до <NUM> миля скорость становиться не так важный тогда как выносливость превращаться в абсолютный необходимость\n",
            "\n",
            "Мутация вносит новую генетическую вариацию, в то время как отбор убирает её из набора проявляющихся вариаций. \n",
            "мутация вносить новый генетический вариация в тот время как отбор убирать её из набор проявляться вариация\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    train_data[0][0],\n",
        "    '\\n' + normalize_text(train_data[0][0], NLP_PIPELINE),\n",
        ")\n",
        "print(\n",
        "    '\\n' + val_data[0][0],\n",
        "    '\\n' + normalize_text(val_data[0][0], NLP_PIPELINE),\n",
        ")\n",
        "print(\n",
        "    '\\n' + test_data[0][0],\n",
        "    '\\n' + normalize_text(test_data[0][0], NLP_PIPELINE),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyTlu5v3zSf9",
        "outputId": "4b5b770b-b430-4bd8-b967-256ece3b8876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Максимальная документная частота (TF-IDF) слова: 0.9714285714285714.\n"
          ]
        }
      ],
      "source": [
        "class_probability = 1.0 / len(classes_list)\n",
        "max_df = 1.0 - 0.2 * class_probability\n",
        "\n",
        "print(f'Максимальная документная частота (TF-IDF) слова: {max_df}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Применим какой-нибудь классификатор"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "ewrytJUsfhER"
      },
      "outputs": [],
      "source": [
        "classifier = Pipeline(steps=[\n",
        "    ('vectorizer', TfidfVectorizer(token_pattern='\\w+', max_df=max_df, min_df=1)),\n",
        "    ('cls', LogisticRegression(solver='saga', max_iter=100, random_state=42))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "OSCmrKkUir99"
      },
      "outputs": [],
      "source": [
        "cv = GridSearchCV(\n",
        "    estimator=classifier,\n",
        "    param_grid={\n",
        "        'vectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "        'cls__C': [1e-1, 1, 10, 100, 1000],\n",
        "        'cls__penalty': ['l1', 'l2']\n",
        "    },\n",
        "    scoring='f1_macro',\n",
        "    cv=5,\n",
        "    refit=True,\n",
        "    n_jobs=multiprocessing.cpu_count()-1,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "lNIJQLu9juHx",
        "outputId": "5383ef97-31a2-4d41-9c83-f987ed6be5a4"
      },
      "outputs": [],
      "source": [
        "cv.fit(\n",
        "    [normalize_text(it, NLP_PIPELINE) for it in train_data[0]],\n",
        "    train_data[1]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyzEID85jzpw",
        "outputId": "ec0d8a90-1b9d-4c34-a067-a97a978ef026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Лучшие параметры: {'cls__C': 1000, 'cls__penalty': 'l2', 'vectorizer__ngram_range': (1, 1)}\n",
            "Лучший показатель F1: 0.6451646278967693\n",
            "Размер вокабуляра: 4359\n"
          ]
        }
      ],
      "source": [
        "print('Лучшие параметры:', cv.best_params_)\n",
        "print('Лучший показатель F1:', cv.best_score_)\n",
        "print('Размер вокабуляра:', len(cv.best_estimator_.named_steps[\"vectorizer\"].vocabulary_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxOgixpvkBFn",
        "outputId": "174270a9-d996-4474-d840-f0ef49165d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "     entertainment       0.80      0.44      0.57         9\n",
            "         geography       0.67      0.75      0.71         8\n",
            "            health       1.00      0.45      0.62        11\n",
            "          politics       0.77      0.71      0.74        14\n",
            "science/technology       0.62      0.80      0.70        25\n",
            "            sports       0.75      0.75      0.75        12\n",
            "            travel       0.57      0.65      0.60        20\n",
            "\n",
            "          accuracy                           0.68        99\n",
            "         macro avg       0.74      0.65      0.67        99\n",
            "      weighted avg       0.71      0.68      0.67        99\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = cv.predict([normalize_text(it, NLP_PIPELINE) for it in val_data[0]])\n",
        "\n",
        "print(classification_report(y_true=val_data[1], y_pred=y_pred, target_names=classes_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQ894gS2lHo0",
        "outputId": "541bd9e6-14e0-4e28-84cb-05ad0a9ac69e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "     entertainment       0.89      0.42      0.57        19\n",
            "         geography       0.64      0.53      0.58        17\n",
            "            health       0.47      0.41      0.44        22\n",
            "          politics       0.78      0.83      0.81        30\n",
            "science/technology       0.65      0.78      0.71        51\n",
            "            sports       0.87      0.80      0.83        25\n",
            "            travel       0.62      0.70      0.66        40\n",
            "\n",
            "          accuracy                           0.68       204\n",
            "         macro avg       0.70      0.64      0.66       204\n",
            "      weighted avg       0.69      0.68      0.68       204\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = cv.predict([normalize_text(it, NLP_PIPELINE) for it in test_data[0]])\n",
        "\n",
        "print(classification_report(y_true=test_data[1], y_pred=y_pred, target_names=classes_list))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
