{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install spacy\n",
    "!python3 -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import spacy\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sub200_ru() -> Tuple[Tuple[List[str], List[int]], Tuple[List[str], List[int]], Tuple[List[str], List[int]], List[str]]:\n",
    "    trainset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='train')\n",
    "    X_train = trainset['text']\n",
    "    y_train = trainset['category']\n",
    "    valset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='validation')\n",
    "    X_val = valset['text']\n",
    "    y_val = valset['category']\n",
    "    testset = load_dataset('Davlan/sib200', 'rus_Cyrl', split='test')\n",
    "    X_test = testset['text']\n",
    "    y_test = testset['category']\n",
    "    categories = set(y_train)\n",
    "    unknown_categories = set(y_val) - categories\n",
    "    if len(unknown_categories) > 0:\n",
    "        err_msg = f'The categories {unknown_categories} are represented in the validation set, but they are not represented in the training set.'\n",
    "        raise RuntimeError(err_msg)\n",
    "    unknown_categories = set(y_test) - categories\n",
    "    if len(unknown_categories) > 0:\n",
    "        err_msg = f'The categories {unknown_categories} are represented in the test set, but they are not represented in the training set.'\n",
    "        raise RuntimeError(err_msg)\n",
    "    categories = sorted(list(categories))\n",
    "    y_train = [categories.index(it) for it in y_train]\n",
    "    y_val = [categories.index(it) for it in y_val]\n",
    "    y_test = [categories.index(it) for it in y_test]\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test), categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s: str, nlp_pipeline: spacy.Language) -> str:\n",
    "    doc = nlp_pipeline(s)\n",
    "    lemmas = [('<NUM>' if token.like_num else token.lemma_.lower()) for token in filter(lambda it1: not it1.is_punct, doc)]\n",
    "    if len(lemmas) == 0:\n",
    "        return ''\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data, classes_list = load_sub200_ru()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['entertainment', 'geography', 'health', 'politics', 'science/technology', 'sports', 'travel']\n"
     ]
    }
   ],
   "source": [
    "print(f'Categories: {classes_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701\n",
      "701\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]))\n",
    "print(len(train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(len(val_data[0]))\n",
    "print(len(val_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "204\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data[0]))\n",
    "print(len(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Турция с трёх сторон окружена морями: на западе — Эгейским, на севере — Чёрным и на юге — Средиземным.\n",
      "турция с <NUM> сторона окружить море на запад эгейский на север чёрный и на юг средиземный\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][0])\n",
    "print(normalize_text(train_data[0][0], nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Если увеличить расстояние для бега с четверти до половины мили, скорость становится не так важна, тогда как выносливость превращается в абсолютную необходимость.\n",
      "если увеличить расстояние для бег с <NUM> до <NUM> миля скорость становиться не так важный тогда как выносливость превращаться в абсолютный необходимость\n"
     ]
    }
   ],
   "source": [
    "print(val_data[0][0])\n",
    "print(normalize_text(val_data[0][0], nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мутация вносит новую генетическую вариацию, в то время как отбор убирает её из набора проявляющихся вариаций.\n",
      "мутация вносить новый генетический вариация в тот время как отбор убирать её из набор проявляться вариация\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0][0])\n",
    "print(normalize_text(test_data[0][0], nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal document frequency of term is 0.9714285714285714.\n"
     ]
    }
   ],
   "source": [
    "class_probability = 1.0 / len(classes_list)\n",
    "max_df = 1.0 - 0.2 * class_probability\n",
    "print(f\"Maximal document frequency of term is {max_df}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer(token_pattern='\\w+', max_df=max_df, min_df=1)),\n",
    "    ('classifier', LogisticRegression(solver='saga', max_iter=100, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GridSearchCV(\n",
    "    estimator=classifier,\n",
    "    # ОРИГИНАЛЬНЫЙ КОД (ЧАСТИЧНО УТЕРЯН, НЕ РАБОТАЕТ):\n",
    "    # param_grid={\n",
    "    #     'cls__C': np.logspace(-4, 4, 9),\n",
    "    #     'cls__penalty': ['l1', 'l2'],\n",
    "    #     'vectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    # },\n",
    "\n",
    "    # КОД, ПРЕДЛОЖЕННЫЙ НЕЙРОНКОЙ (РАБОТАЕТ, НО В ОРИГИНАЛЕ БЫЛ ДРУГОЙ):\n",
    "    param_grid={\n",
    "        'vectorizer__max_df': np.arange(0.80, 1.00, 0.01),\n",
    "        'vectorizer__min_df': np.arange(0.001, 0.01, 0.001),\n",
    "    },\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit([normalize_text(it, nlp) for it in train_data[0]], train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'vectorizer__max_df': 0.8, 'vectorizer__min_df': 0.006}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters:')\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sest Fl-macro:\n",
      "0.4577708076667141\n"
     ]
    }
   ],
   "source": [
    "print('Sest Fl-macro:')\n",
    "print(cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size is:\n",
      "484\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size is:')\n",
    "print(len(cv.best_estimator_.named_steps[\"vectorizer\"].vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     entertainment       1.00      0.22      0.36         9\n",
      "         geography       0.80      0.50      0.62         8\n",
      "            health       1.00      0.27      0.43        11\n",
      "          politics       0.58      0.50      0.54        14\n",
      "science/technology       0.48      0.84      0.61        25\n",
      "            sports       0.67      0.50      0.57        12\n",
      "            travel       0.42      0.50      0.45        20\n",
      "\n",
      "          accuracy                           0.54        99\n",
      "         macro avg       0.71      0.48      0.51        99\n",
      "      weighted avg       0.63      0.54      0.52        99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = cv.predict([normalize_text(it, nlp) for it in val_data[0]])\n",
    "print(classification_report(y_true=val_data[1], y_pred=y_pred, target_names=classes_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
